% This will throw an error so you will look here and look at your notes
% below
%abc =valNothere
% see
% C:\Users\Philip\Documents\MATLAB\Fall2018\DesignOpt\checkNN\matlab-mnist-two-layer-perceptron-master\matlab-mnist-two-layer-perceptron-master
%% Philip Hoddinott NN
% Neural Net for MNIST numbers
%% Setup
% Setup Enviorment, get data
% Note this may take a while
clear all; close all;


inputValues = loadMNISTImages('train-images.idx3-ubyte');
labels = loadMNISTLabels('train-labels.idx1-ubyte');



% Transform the labels to correct target values.
targetValues = 0.*ones(10, size(labels, 1));
for n = 1: size(labels, 1)
    targetValues(labels(n) + 1, n) = 1;
end;
%{
imageWidth = 28;
imageHeight = 28;
inputSize = imageWidth*imageHeight;

load('I_train.mat');
load('L_T_labels.mat');
xTrain=images';

load('I_test.mat');
xTest=images';

load('L_Tst_labels.mat');
Yvals=tTrain;
Xvals=xTrain;

YvalsM=Yvals;
XvalsM=Xvals;
%}
nn_input_dim=784;
nn_hdim=250;
nn_output_dim=10;

numberOfHiddenUnits=nn_hdim;
learningRate=.1;
chunk=1; % creat esub arrays
%pNet=philipNet(nn_input_dim,nn_hdim,nn_output_dim,learningRate,chunk);
pNet=philipNet2(inputValues,targetValues,numberOfHiddenUnits);


inputValuesTest = loadMNISTImages('t10k-images.idx3-ubyte');
labelsTest = loadMNISTLabels('t10k-labels.idx1-ubyte');
activationFunction = @ActFunc;
dActivationFunction = @dActFunc;
batchSize = 100;
epochs = 500;

fprintf('Train twolayer perceptron with %d hidden units.\n', nn_hdim);
fprintf('Learning rate: %d.\n', learningRate);
%[pNet,error] = handleTrainNet(pNet, activationFunction, dActivationFunction,inputValues, targetValues, epochs, batchSize,learningRate)
%[pNet,error] = handleTrainNet2(pNet, activationFunction, dActivationFunction,inputValues, targetValues, epochs, batchSize,learningRate)
[pNet,error]=handleTrainNet2(pNet,activationFunction, dActivationFunction, numberOfHiddenUnits, inputValues, targetValues, epochs, batchSize, learningRate);
%[pNet,error]=handleTrainNet3(pNet,activationFunction, dActivationFunction, numberOfHiddenUnits, inputValues, targetValues, epochs, batchSize, learningRate);
fprintf('Validation:\n');

[correctlyClassified, classificationErrors] = testAcc(activationFunction, pNet, inputValuesTest, labelsTest);

fprintf('Classification errors: %d\n', classificationErrors);
fprintf('Correctly classified: %d\n', correctlyClassified);
acc=100*(correctlyClassified)/(correctlyClassified+classificationErrors);
fprintf('accuracy: %.5f\n', acc);
save('wkspc_1')
keyboard
  
XvalCr={};
YvalCr={};
for i=1:length(XvalsM(:,1))/chunk
    sVal=1+(i-1)*chunk;
    eVal=(i)*chunk;
    XvalCr(i)={XvalsM(sVal:eVal,:)};
    YvalCr(i)={YvalsM(sVal:eVal,:)};
end
%% Check stuff

iCt=1;

iCy=1;
h = animatedline;
xpt=1:1:length(XvalsM(:,1));
incrmenter=10;
accM=[];
ac=-1;
testAc=-1;
bestAc=-1;
figure(1)
grid on
tic
pltCount=1;
testAcOld=-1;
modelCurModel=nNet;
while iCy<10   || testAc<94
    nNet=modelCurModel;
    Xvals=cell2mat(XvalCr(iCt));
    Yvals=cell2mat(YvalCr(iCt));
    [modelCurModel]   = trainSB(nNet,Xvals,Yvals);
        
        a2=nNet.a2;
        %size(a3)
        %size(Yvals)
        lossV=sum(softmax_loss(Yvals,a2));
        y_hat = predict(modelCurModel,Xvals);
        [M,y_true] = max(Yvals,[],2);
        ac=accuracy_score(y_hat,y_true);
        [testAc] = TestNN_func(modelCurModel,xTest,tTest);
    if testAc>bestAc
        bestAc=testAc;
        modelBest=modelCurModel;
    end
    if  mod(iCt,incrmenter)==0
        lossM(pltCount,iCy)=lossV;
        
        accM(pltCount,iCy)=ac;

        addpoints(h,xpt(iCt),testAc);
        st=sprintf('Acc = %.2f, best Acc = %.3f',testAc,bestAc);
        title(st)
        drawnow
        
        fprintf('ls aft %d=%.4e, lr = %.4e',iCt,lossV,learningRate);

        fprintf('lop %d,tstAc = %.2f, bstTac = %.2f, curAc: %d, ',iCy,testAc,bestAc,ac);
        toc
        pltCount=pltCount+1;
             
    end
    
    iCt=iCt+1;
    if iCt>i
        if testAc<testAcOld

        end

        iCt=1;
        pltCount=1;
        fprintf('Back again, ');
        iCy=iCy+1;
        [testAc] = TestNN_func(modelCurModel,xTest,tTest);
        fprintf('Test Set accuracy = %.3f\n',testAc);
                   
        
        %st=sprintf('ogLR = %.5e, lr = %.5e,, last lr = %.5e',learning_rateOG,learning_rate,testAcOld);
        testAcOld=testAc;
        learning_rateOld=learningRate;
        %title(st)
        modelCurModel=modelBest;
    end
    
    if testAc>bestAc
        bestAc=testAc;
        modelBest=modelCurModel;
    end
end
save('wkspce');

function retV = softmax(z)
    exp_scores = exp(z);
    retV= exp_scores./(sum(exp_scores,2));

end
function loss = softmax_loss(y,y_hat)
    minval = 0.000000000001;
    m=length(y);
    y_hatCp=y_hat;
    y_hatCp(y_hatCp<minval)=minval;

    if ~isempty(y_hatCp)

        if y_hatCp~=y_hat
            fprintf('clip\n');

            y_hat=y_hatCp;
        end
    end
    %keyboard
    loss= -1/m * sum(y.*log(y_hat));

end
function lossDeriv = loss_derivative(y,y_hat)
    lossDeriv=y_hat-y;
end
function tndv = tanh_derivative(x)
    tndv=4./((exp(-x)+exp(x)).^2);
end

function funcD=dActFunc(x)
    % tndv=4./((exp(-x)+exp(x)).^2); % tan h deriv 
     funcD = ActFunc(x).*(1 - ActFunc(x));
end

function funcVal = ActFunc(x)
    funcVal = 1./(1 + exp(-x)); % logistic sigmoid
     %y = 1./(1 + exp(-x));
     %funcVal=y;
end
 

function nNet= forward_prop(nNet,a0)
    W1=nNet.W1;
    W2=nNet.W2;
 
    b1=nNet.b1;
    b2=nNet.b2;

    z1=(a0*W1) +b1;
    a1=ActFunc(z1);
    
    
    z2=(a1*W2)+b2;
    a2 = softmax(z2); 

    nNet.a0=a0;
    nNet.a1=a1;
    nNet.a2=a2;

    nNet.z1=z1;
    nNet.z2=z2;

end

function nNet= backwards_prop(nNet,y)
    W1=nNet.W1;
    W2=nNet.W2;
    
    b1=nNet.b1;
    b2=nNet.b2;
    
    a0=nNet.a0;
    a1=nNet.a1;
    a2=nNet.a2;
    
    
    z1=nNet.z1;
    z2=nNet.z2;
    
    m=length(y);
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BIG NOTE
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %keyboard;
    %loss_derivative(y,a3);
    dz2 =loss_derivative(y,a2);

    dW2 = (1/m)*((a1')*dz2);
    
    db2 = (1/m)*sum(dz2);

    
    dz1 = ((dz2*(W2')) .*dActFunc(z1));
    
    dW1 = (1/m)*((a0')*dz1);
    
    db1 = (1/m)*sum(dz1);
    
    nNet.dW1=dW1;
    nNet.dW2=dW2;
    
    nNet.db1=db1;
    nNet.db2=db2;
    
end

function nNet = load_parameters(netBest,nn_input_dim,nn_hdim,nn_output_dim,learning_rate,chunk)
    nNet=philipNet_1(nn_input_dim,nn_hdim,nn_output_dim,learning_rate,chunk);
    rng('shuffle')
    
    nNet.W1=netBest.W1;
    nNet.W2=netBest.W2;
    
    nNet.b1=netBest.b1;
    nNet.b2=netBest.b2;
    
    
end

function accuracy_scoreVal = accuracy_score(y_hat,y_true)
    accP=0;
    for ik =1:length(y_hat)
        if y_hat(ik,1)==y_true(ik,1)
            accP=accP+1;
        end
    end
    accPct=100*accP/ik;
    accuracy_scoreVal=accPct;

    
end

function nNet = update_parameters(nNet)

    W1=nNet.W1;
    W2=nNet.W2;
    
    b1=nNet.b1;
    b2=nNet.b2;
    
    learning_rate=nNet.learning_rate;

    W1 =W1- learning_rate * nNet.dW1;
    b1 =b1- learning_rate * nNet.db1;
    
    W2 =W2- learning_rate * nNet.dW2;
    b2 =b2- learning_rate * nNet.db2;
    

    nNet.W1=W1;
    nNet.W2=W2;
    
    nNet.b1=b1;
    nNet.b2=b2;
    
end

function y_hat = predict(model,x)
    % Do forward pass
    c = forward_prop(model,x);%l
    [M,y_hat]=max(c.a2,[],2);

end



function [nNet]   = trainSB(nNet,Xp,yp,epoch)
    for i=1:epoch
        nNet= forward_prop(nNet,Xp);
        nNet = backwards_prop(nNet,yp);
        nNet = update_parameters(nNet);
    end
end
function [pNet,error] = handleTrainNet3(pNet,activationFunction, dActivationFunction, numberOfHiddenUnits, inputValues, targetValues, epochs, batchSize, learningRate)
    figure
    hold on;
    %trainingSetSize=60000;
    % The number of training vectors.
    trainingSetSize = size(inputValues, 2);
    
    % Input vector has 784 dimensions.
    inputDimensions = size(inputValues, 1);
    % We have to distinguish 10 digits.
    outputDimensions = size(targetValues, 1);
    
    % Initialize the weights for the hidden layer and the output layer.
    hiddenWeights = rand(numberOfHiddenUnits, inputDimensions);
    outputWeights = rand(outputDimensions, numberOfHiddenUnits);
    
    hiddenWeights = hiddenWeights./size(hiddenWeights, 2);
    outputWeights = outputWeights./size(outputWeights, 2);
    
    n = zeros(batchSize);
    errorOld=100;
    
    figure; hold on;

    for t = 1: epochs
        for k = 1: batchSize
            % Select which input vector to train on.
            n(k) = floor(rand(1)*trainingSetSize + 1);
            
            % Propagate the input vector through the network.
            inputVector = inputValues(:, n(k));
            hiddenActualInput = hiddenWeights*inputVector;
            hiddenOutputVector = activationFunction(hiddenActualInput);
            outputActualInput = outputWeights*hiddenOutputVector;
            outputVector = activationFunction(outputActualInput);
            
            targetVector = targetValues(:, n(k));
            
            % Backpropagate the errors.
            outputDelta = dActivationFunction(outputActualInput).*(outputVector - targetVector);
            hiddenDelta = dActivationFunction(hiddenActualInput).*(outputWeights'*outputDelta);
            
            outputWeights = outputWeights - learningRate.*outputDelta*hiddenOutputVector';
            hiddenWeights = hiddenWeights - learningRate.*hiddenDelta*inputVector';
            %keyboard;
        end
        
        % Calculate the error for plotting.
        error = 0;
        for k = 1: batchSize
            inputVector = inputValues(:, n(k));
            targetVector = targetValues(:, n(k));
            
            error = error + norm(activationFunction(outputWeights*activationFunction(hiddenWeights*inputVector)) - targetVector, 2);
        end
        error = error/batchSize;
        plot(t, error,'*');
        drawnow
    end
    
        
end


function [pNet,error] = handleTrainNet2(pNet,activationFunction, dActivationFunction, numberOfHiddenUnits, inputValues, targetValues, epochs, batchSize, learningRate)
    figure
    hold on;
    %trainingSetSize=60000;
    % The number of training vectors.
    trainingSetSize = size(inputValues, 2);
    
    % Input vector has 784 dimensions.
    inputDimensions = size(inputValues, 1);
    % We have to distinguish 10 digits.
    outputDimensions = size(targetValues, 1);
    
    % Initialize the weights for the hidden layer and the output layer.
    hiddenWeights = rand(numberOfHiddenUnits, inputDimensions);
    outputWeights = rand(outputDimensions, numberOfHiddenUnits);
    
    hiddenWeights = hiddenWeights./size(hiddenWeights, 2);
    outputWeights = outputWeights./size(outputWeights, 2);
    
    n = zeros(batchSize);
    errorOld=100;
     h = animatedline('Color','r');
    figure; hold on;
    subplot(2,1,1)
    plot(1, 1,'*');
    grid on;
    title('Error')
    subplot(2,1,2)
    addpoints(h,,10);
    title(
    for t = 1: epochs
        for k = 1: batchSize
            % Select which input vector to train on.
            n(k) = floor(rand(1)*trainingSetSize + 1);
            
            % Propagate the input vector through the network.
            inputVector = inputValues(:, n(k));
            hiddenActualInput = pNet.hiddenWeights*inputVector;
            hiddenOutputVector = activationFunction(hiddenActualInput);
            outputActualInput = pNet.outputWeights*hiddenOutputVector;
            outputVector = activationFunction(outputActualInput);
            
            targetVector = targetValues(:, n(k));
            
            % Backpropagate the errors.
            outputDelta = dActivationFunction(outputActualInput).*(outputVector - targetVector);
            hiddenDelta = dActivationFunction(hiddenActualInput).*(pNet.outputWeights'*outputDelta);
            
            pNet.outputWeights = pNet.outputWeights - learningRate.*outputDelta*hiddenOutputVector';
            pNet.hiddenWeights = pNet.hiddenWeights - learningRate.*hiddenDelta*inputVector';
            %keyboard;
        end
        
        % Calculate the error for plotting.
        error = 0;
        for k = 1: batchSize
            inputVector = inputValues(:, n(k));
            targetVector = targetValues(:, n(k));
            
            error = error + norm(activationFunction(pNet.outputWeights*activationFunction(pNet.hiddenWeights*inputVector)) - targetVector, 2);
        end
        error = error/batchSize;
        subplot(2,1,1)
        plot(t, error,'*');
        grid on;
        title('Error')
        subplot(2,1,2)
        addpoints(h,t,acc(t));
        drawnow
    end
    
        
end


function [pNet,error] = handleTrainNet(pNet, activationFunction, dActivationFunction,inputValues, targetValues, epochs, batchSize,learningRate)
    figure
    hold on;
    trainingSetSize=60000;
    for t=1:epochs
        for k=1:batchSize
            n(k) = floor(rand(1)*trainingSetSize + 1);
            % forward propgation
            inputVector = inputValues(:, n(k));
            pNet.b1=(pNet.W1')*inputVector;
            pNet.a1=activationFunction(pNet.b1);
            pNet.b2 = (pNet.W2')* pNet.a1;
            pNet.a2 = activationFunction(pNet.b2);
            
            
            targetVector = targetValues(:, n(k));
            % backward propgation
            %keyboard;
            pNet.db1 = dActivationFunction(pNet.b2).*(pNet.a2 - targetVector);
            pNet.db2 = dActivationFunction( pNet.b1).*((pNet.W2')'* pNet.db1);
            
            pNet.W2 = (pNet.W2') - learningRate.*pNet.db1*(pNet.a1');
            pNet.W1 = (pNet.W1') - learningRate.*pNet.db2*inputVector';
            %  outputWeights = outputWeights - learningRate.*outputDelta*hiddenOutputVector';
            %hiddenWeights = hiddenWeights - learningRate.*hiddenDelta*inputVector';
            pNet.W1=pNet.W1';
            pNet.W2=pNet.W2';
        end
        %keyboard
        %size((pNet.W1')*inputVector)
         %size(targetVector)
         %size(pNet.W1')
        error = 0;
        for k = 1: batchSize
            inputVector = inputValues(:, n(k));
            targetVector = targetValues(:, n(k));
            %error = error + norm(activationFunction(outputWeights*activationFunction(hiddenWeights*inputVector)) - targetVector, 2);
            error = error + norm(activationFunction(pNet.W2'*activationFunction( (pNet.W1')*inputVector)) - targetVector, 2);
        end
        error = error/batchSize;
        plot(t, error,'*');
        drawnow
    end
        
end

%function pNet = trainPhilipNet(pNet, activationFunction, dActivationFunction, epochs, batchSize)
%     n = zeros(batchSize);

